<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
        <title>Eli Qiao's Blog</title>
        <description>Eli Qiao's Blog - Eli Qiao</description>
        <link>http://taget.github.io</link>
        <link>http://taget.github.io</link>
        <lastBuildDate>2018-11-28T17:18:17+08:00</lastBuildDate>
        <pubDate>2018-11-28T17:18:17+08:00</pubDate>
        <ttl>1800</ttl>


        <item>
                <title>TCP connection</title>
                <description>
&lt;h2 id=&quot;tcp&quot;&gt;TCP&lt;/h2&gt;

&lt;p&gt;TCP 是传输层的一个协议，用来提供可靠传输。&lt;/p&gt;

&lt;p&gt;TCP 报文定义见下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tcp_package.gif&quot; alt=&quot;tcp package&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TCP 与 IP 协议一起工作，用于应用进程之间的通信。&lt;/p&gt;

&lt;h2 id=&quot;tcp-的基本概念&quot;&gt;TCP 的基本概念&lt;/h2&gt;

&lt;p&gt;三次握手与四次挥手&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tcp_3_4.jpeg&quot; alt=&quot;tcp package&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;三次握手&quot;&gt;三次握手&lt;/h3&gt;

&lt;p&gt;由客户端发起连接请求，服务端相应请求并向客户端发起请求确认，客户端返回确认请求。
此时连接建立。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;第一次握手：建立连接。客户端发送连接请求报文段，将SYN位置为1，Sequence Number为x；然后，客户端进入SYN_SEND状态，等待服务器的确认；&lt;/li&gt;
  &lt;li&gt;第二次握手：服务器收到SYN报文段。服务器收到客户端的SYN报文段，需要对这个SYN报文段进行确认，设置Acknowledgment Number为x+1(Sequence Number+1)；同时，自己自己还要发送SYN请求信息，将SYN位置为1，Sequence Number为y；服务器端将上述所有信息放到一个报文段（即SYN+ACK报文段）中，一并发送给客户端，此时服务器进入SYN_RECV状态；&lt;/li&gt;
  &lt;li&gt;第三次握手：客户端收到服务器的SYN+ACK报文段。然后将Acknowledgment Number设置为y+1，向服务器发送ACK报文段，这个报文段发送完毕以后，客户端和服务器端都进入ESTABLISHED状态，完成TCP三次握手。
完成了三次握手，客户端和服务器端就可以开始传送数据。以上就是TCP三次握手的总体介绍。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。
采用“三次握手”的办法可以防止server由于重传导致的老的报文一直等待client发来数据。这样，server的很多资源就白白浪费掉了现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。”。主要目的防止server端一直等待，浪费资源。&lt;/p&gt;

&lt;h3 id=&quot;四次挥手&quot;&gt;四次挥手&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tcp_4_fin.png&quot; alt=&quot;tcp package&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TOOD&lt;/p&gt;

&lt;h3 id=&quot;socket-各种状态分析&quot;&gt;Socket 各种状态分析&lt;/h3&gt;

&lt;p&gt;TODO&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TIME_WAIT&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://taget.github.io/2017/11/27/tcp</link>
                <guid>http://taget.github.io/2017/11/27/tcp</guid>
                <pubDate>2017-11-27T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Vote Url</title>
                <description>&lt;h2 id=&quot;tokyo-summit-vote-url&quot;&gt;Tokyo summit vote url&lt;/h2&gt;

&lt;p&gt;您可以直接点击以下链接进行投票。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/Presentation/4971&quot; target=&quot;_blank&quot;&gt;Towards Robust Live Migration in Dynamic Environments&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/Presentation/4930&quot; target=&quot;_blank&quot;&gt;OpenStack Practice @ Intel IT: Let’s accelerate the design process of smart devices&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/Presentation/6679&quot; target=&quot;_blank&quot;&gt;CI/CDaaS based on Magnum/Murano within Intel IT Private Cloud&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/4610&quot; target=&quot;_blank&quot;&gt;OpenStack DevOps in Intel IT: How to serve a private Cloud while continues contribute to community&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/4330&quot; target=&quot;_blank&quot;&gt;Media Cloud: Evolve Online Video Business with Full Graphics Virtualization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/5749&quot; target=&quot;_blank&quot;&gt;Telemetry for NFV in OpenStack&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/6646&quot; target=&quot;_blank&quot;&gt;Murano-based DevOps Best Practice for SaaS Applications&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/6653&quot; target=&quot;_blank&quot;&gt;Sleep Easy with Automated Cinder Volume Backup&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/6637&quot; target=&quot;_blank&quot;&gt;Mutual backup across data centers based oncinder and Ceph&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/5595&quot; target=&quot;_blank&quot;&gt;Distributed Health Checking for Compute Node High Availability&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/5835&quot; target=&quot;_blank&quot;&gt;Bridging virtual networks from different OpenStack based clouds&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/5474&quot; target=&quot;_blank&quot;&gt;Event based alarming in Ceilometer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/4984&quot; target=&quot;_blank&quot;&gt;High Performance Cost-Effective Virtual Desktops with OpenStack&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/6427&quot; target=&quot;_blank&quot;&gt;Implement of live resize for kvm vms&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.openstack.org/summit/tokyo-2015/vote-for-speakers/presentation/6642&quot; target=&quot;_blank&quot;&gt;Testing rolling upgrade using Grenade with Kolla&lt;/a&gt;&lt;/p&gt;
</description>
                <link>http://taget.github.io/lessons/2015/07/26/vote-url</link>
                <guid>http://taget.github.io/lessons/2015/07/26/vote-url</guid>
                <pubDate>2015-07-26T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Hacking Magnum Swarm Proxy</title>
                <description>&lt;h2 id=&quot;hacking-magnum-swarm-node-to-add-proxy&quot;&gt;Hacking magnum swarm node to add proxy&lt;/h2&gt;

&lt;p&gt;I will talk about how to add proxy in magnum swarm framework&lt;/p&gt;

&lt;h3 id=&quot;why-i-need-proxy&quot;&gt;Why I need proxy&lt;/h3&gt;

&lt;p&gt;Because GFW, you know!&lt;/p&gt;

&lt;h3 id=&quot;where-to-add-proxy&quot;&gt;Where to add proxy&lt;/h3&gt;

&lt;p&gt;We need an http_proxy and https_proxy first! if you do not have that proxy, then you can leave now.&lt;/p&gt;

&lt;h4 id=&quot;dockerd-need-proxy-to-pull-image&quot;&gt;dockerd need proxy to pull image&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir /etc/systemd/system/docker.service.d/

$ cat /etc/systemd/system/docker.service.d/proxy.conf &amp;amp;lt;&amp;amp;lt; EOF

[Service]

Environment=HTTP_PROXY=http://x.x.x.x:yy

EOF

$ systemctl daemon-reload

 systemctl restart docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;docker-service-need-proxy-to-access-extrenl-network&quot;&gt;docker service need proxy to access extrenl network&lt;/h4&gt;

&lt;p&gt;we can passing to docker run command line like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-e http_proxy=http://x.x.x.x:yy \
-e https_proxy=http://x.x.x.x:yy \
-e no_proxy=192.168.0.3,192.168.0.4,192.168.0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;no_proxy is important, since swarm node internal access will use 192* ip.&lt;/p&gt;

&lt;h4 id=&quot;docker-host-need-proxy&quot;&gt;docker host need proxy&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;cat /etc/bashrc &amp;amp;lt;&amp;amp;lt;EOF
declare -x http_proxy=http://x.x.x.x:yy
declare -x https_proxy=http://x.x.x.x:yy
declare -x no_proxy=192.168.0.3,192.168.0.4,192.168.0.5
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;notes we only add 192.168.0.3,192.168.0.4,192.168.0.5 because we only have 3 nodes and their ips are them, this is hacking, we need to dynimic adding them this is okay, because we can get them from HEAT template. Since I am hacking, it is okay. I know what I am doing.&lt;/p&gt;

&lt;h3 id=&quot;hacking-magnum-code-to-adding-proxy&quot;&gt;hacking magnum code to adding proxy&lt;/h3&gt;

&lt;p&gt;I put all of them into a patch, you may get it from here, but please not it is only working for my enviroment, just an example for you.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/taget/mybin/blob/master/misc/0001-Add-proxy-swarm.patch&quot;&gt;patch&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;go-and-play-with-it&quot;&gt;go and play with it&lt;/h3&gt;

&lt;p&gt;And you need to have an OpenStack setup, command line to create baymodelas follows&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nova keypair-add --pub-key ~/.ssh/id_rsa.pub testkey
NIC_ID=$(neutron net-show public | awk '/ id /{print $4}')

# create a baymodel
magnum baymodel-create --name swarmmodel --image-id fedora-21-atomic-3 \
    --keypair-id testkey --external-network-id $NIC_ID \
    --dns-nameserver 10.248.2.5 --flavor -id m1.small \
    --docker-volume-size 5 --coe swarm --fixed-network 192.168.0.0/24

# create swarm bay which has 2 agent nodes and 1 manager node
magnum bay-create --name swarmbay --baymodel swarmmodel --node-count 2

magnum container-create --name hello --image cirros --bay swarmbay --command &quot;echo hello world&quot;
magnum container-list
+--------------------------------------+---------------+---------+
| uuid                                 | name          | status  |
+--------------------------------------+---------------+---------+
| 084a650f-e17c-4aab-b229-74567d38f4e7 | testcontainer | Stopped |
| f1d7a992-8073-44cd-966f-6e08d102cc1b | hello         | Stopped |
+--------------------------------------+---------------+---------+
&lt;/code&gt;&lt;/pre&gt;

</description>
                <link>http://taget.github.io/lessons/2015/07/03/hacking-magnum-swarm-proxy</link>
                <guid>http://taget.github.io/lessons/2015/07/03/hacking-magnum-swarm-proxy</guid>
                <pubDate>2015-07-03T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Vhost net</title>
                <description>
&lt;h1 id=&quot;vhost-概述&quot;&gt;Vhost 概述&lt;/h1&gt;
&lt;p&gt;Linux kernel 中的vhost driver提供了KVM在kernel环境中的virtio设备的模拟。vhost把QEMU模拟设备的代码放在了linux kernel里面，所以设备模拟代码可以直接进入kernel子系统，从而不需要从用户空间通过系统调用陷入内核，减少了由于模拟IO导致的性能下降。&lt;/p&gt;

&lt;p&gt;vhost-net是在宿主机上对vhost 网卡的模拟，同样，也有vhost-blk，对block设备的模拟，以及vhost-scsi，对scsi设备的模拟。&lt;/p&gt;

&lt;p&gt;vhost 在kernel中的代码位于 drivers/vhost/vhost.c&lt;/p&gt;

&lt;h2 id=&quot;vhost-驱动模型&quot;&gt;Vhost 驱动模型&lt;/h2&gt;

&lt;p&gt;vhost driver创建了一个字符设备 /dev/vhost-net，这个设备可以被用户空间打开，并可以被ioctl命令操作。当给一个Qemu进程传递了参数-netdev tap,vhost=on 的时候，QEMU会通过调用几个ioctl命令对这个文件描述符进行一些初始化的工作，然后进行特性的协商，从而宿主机跟客户机的vhost-net driver建立关系。
QEMU代码调用如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vhost_net_init -&amp;gt; vhost_dev_init -&amp;gt; vhost_net_ack_features
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在vhost_net_init中调用了 vhost_dev_init ，打开/dev/vhost-net这个设备，然后返回一个文件描述符作为vhost-net的后端，
vhost_dev_init 调用的ioctl命令有&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r = ioctl(hdev-&amp;gt;control, VHOST_SET_OWNER, NULL);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kernel 中的定义为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.  /* Set current process as the (exclusive) owner of this file descriptor.  This  
2.   * must be called before any other vhost command.  Further calls to  
3.   * VHOST_OWNER_SET fail until VHOST_OWNER_RESET is called. */  
4.  #define VHOST_SET_OWNER _IO(VHOST_VIRTIO, 0x01)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后获取VHOST支持的特性&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;r = ioctl(hdev-&amp;gt;control, VHOST_GET_FEATURES, &amp;amp;features);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样，kernel中的定义为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.  /* Features bitmask for forward compatibility.  Transport bits are used for  
2.   * vhost specific features. */  
3.  #define VHOST_GET_FEATURES      _IOR(VHOST_VIRTIO, 0x00, __u64)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;QEMU中用vhost_net 这个数据结构代表打开的vhost_net 实例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.  struct vhost_net {  
2.      struct vhost_dev dev;  
3.      struct vhost_virtqueue vqs[2];  
4.      int backend;  
5.      NetClientState *nc;  
6.  };  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用ioctl设置完后，QEMU注册memory_listener 回调函数:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.  hdev-&amp;gt;memory_listener = (MemoryListener) {  
2.      .begin = vhost_begin,  
3.      .commit = vhost_commit,  
4.      .region_add = vhost_region_add,  
5.      .region_del = vhost_region_del,  
6.      .region_nop = vhost_region_nop,  
7.      .log_start = vhost_log_start,  
8.      .log_stop = vhost_log_stop,  
9.      .log_sync = vhost_log_sync,  
10.     .log_global_start = vhost_log_global_start,  
11.     .log_global_stop = vhost_log_global_stop,  
12.     .eventfd_add = vhost_eventfd_add,  
13.     .eventfd_del = vhost_eventfd_del,  
14.     .priority = 10  
15. };  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vhost_region_add 是为了将QEMU guest的地址空间映射到vhost driver&lt;/p&gt;

&lt;p&gt;最后进行特性的协商:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.  /* Set sane init value. Override when guest acks. */  
2.  vhost_net_ack_features(net, 0); 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;与此同时，kernel中要创建一个kernel thread 用于处理I/O事件和设备的模拟。
kernel代码 drivers/vhost/vhost.c：&lt;/p&gt;

&lt;p&gt;在vhost_dev_set_owner中，调用了这个函数用于创建worker线程（线程名字为vhost-qemu+进程pid）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/vhost-net.jpg&quot; alt=&quot;arc of vhost&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;kernel-中的virtio-模拟&quot;&gt;Kernel 中的virtio 模拟&lt;/h2&gt;

&lt;p&gt;vhost并没有完全模拟一个pci设备，相反，它只把自己限制在对virtqueue的操作上。&lt;/p&gt;

&lt;p&gt;worker thread 一直等待virtqueue的数据，对于vhost-net来说，当virtqueue的tx队列中有数据了，它会把数据传送到与其相关联的tap设备文件描述符上。&lt;/p&gt;

&lt;p&gt;相反，worker thread 也要进行tap文件描述符的轮询，对于vhost-net，当tap文件描述符有数据到来时候，worker thread会被唤醒，然后将数据传送到rx队列中。&lt;/p&gt;

&lt;h2 id=&quot;vhost的在用户空间的接口&quot;&gt;vhost的在用户空间的接口&lt;/h2&gt;

&lt;p&gt;数据已经准备好了，如何通知客户机呢？&lt;/p&gt;

&lt;p&gt;从vhost模块的依赖性可以得知，vhost这个模块并没有依赖于kvm 模块，也就是说，理论上其他应用只要实现了和vhost接口，也可以调用vhost来进行数据传输的加速。&lt;/p&gt;

&lt;p&gt;但也正是因为vhost跟kvm模块没什么关系，当QEMU（KVM）guest把数据放到tx队列（virtqueue）上之后，它是没有办法直接通知vhost数据准备了的。&lt;/p&gt;

&lt;p&gt;不过，vhost 设置了一个eventfd文件描述符，这个文件描述符被前面我们提到的worker thread 监控，所以QEMU可以通过向eventfd发送消息告诉vhost数据准备好了。&lt;/p&gt;

&lt;p&gt;QEMU的做法是这样的，在QEMU中注册一个ioeventfd，当guest 发生I/O退出了，会被KVM捕捉到，KVM向vhost发送eventfd从而告知vhost KVM guest已经准备好数据了。由于 worker thread监控这个eventfd，在收到消息后，知道guest已经把数据放到了tx队列，可以进行对戏&lt;/p&gt;

&lt;p&gt;vhost通过发出一个guest中断，通过KVM提供的irqevent，告诉guest需要传送的buffer已经放到了rx virtqueue了，QEMU（KVM）注册这个irq PCI事件，得知内核空间的数据准备好了，调用guest驱动进行数据的读取。&lt;/p&gt;

&lt;p&gt;所以，总的来说 vhost 实例需要知道的有三样&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;guest的内存映射，也就是virtqueue，用于数据的传输&lt;/li&gt;
  &lt;li&gt;qemu kick eventfd，vhost接收guest发送的消息，该消息被worker thread捕获&lt;/li&gt;
  &lt;li&gt;call event（irqfd）用于通知guest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上三点在QEMU初始化的时候准备好，数据的传输只在内核空间就完成了，不需要QEMU进行干预，所以这也是为什么使用vhost进行传输数据高效的原因。&lt;/p&gt;

</description>
                <link>http://taget.github.io/2014/09/23/vhost</link>
                <guid>http://taget.github.io/2014/09/23/vhost</guid>
                <pubDate>2014-09-23T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Request Data Flow</title>
                <description>&lt;h2 id=&quot;data-flow-of-openstack-request&quot;&gt;Data flow of openstack request&lt;/h2&gt;

&lt;p&gt;This post will talk about the data flow of an openstack request, I will take boot an instance and shelve an instance as example.&lt;/p&gt;

&lt;p&gt;Assume that you have setup an openstack environment already.&lt;/p&gt;

&lt;p&gt;Let’s start from nova client, you can put &lt;code&gt;–debug&lt;/code&gt; option to nova client then you will see the request url nova client sent.&lt;/p&gt;

&lt;h3 id=&quot;boot-an-instance&quot;&gt;Boot an instance&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;[tagett@stack-01 devstack]$ nova --debug boot t3 --flavor m1.nano --image  44c37b90-0ec3-460a-bdf2-bd8bb98c9fdf --nic net-id=b745b2c6-db16-40ab-8ad7-af6da0e5e699
…
REQ: curl -i 'http://cloudcontroller:5000/v2.0/tokens'
REQ: curl -i 'http://cloudcontroller:8774/v2/d7beb7f28e0b4f41901215000339361d/images/44c37b90-0ec3-460a-bdf2-bd8bb98c9fdf'
REQ: curl -i 'http://cloudcontroller:8774/v2/d7beb7f28e0b4f41901215000339361d/flavors/m1.nano'
REQ: curl -i 'http://cloudcontroller:8774/v2/d7beb7f28e0b4f41901215000339361d/servers' -X POST -H &quot;Accept: application/json&quot; -H &quot;Content-Type: application/json&quot; -H &quot;User-Agent: python-novaclient&quot; -H &quot;X-Auth-Project-Id: admin&quot; -H &quot;X-Auth-Token: {SHA1}15d9e554b7456f1043732bb8df72d1521c5f6aa1&quot; -d '{&quot;server&quot;: {&quot;name&quot;: &quot;t3&quot;, &quot;imageRef&quot;: &quot;44c37b90-0ec3-460a-bdf2-bd8bb98c9fdf&quot;, &quot;flavorRef&quot;: &quot;42&quot;, &quot;max_count&quot;: 1, &quot;min_count&quot;: 1, &quot;networks&quot;: [{&quot;uuid&quot;: &quot;b745b2c6-db16-40ab-8ad7-af6da0e5e699&quot;}]}}'
…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The requests from nova client are:&lt;/p&gt;

&lt;p&gt;1)  Get token from keystone server with username and password, keystone will return the token, and also the tenant id.&lt;/p&gt;

&lt;p&gt;2)  Send another request with tenant id (d7beb7f28e0b4f41901215000339361d) and token to check if image existed.&lt;/p&gt;

&lt;p&gt;3)  Send another request with tenant id (d7beb7f28e0b4f41901215000339361d) and token to check if favors existed.&lt;/p&gt;

&lt;p&gt;4)  Request to create an instance, all request data are put in the request body, check –d section.&lt;/p&gt;

&lt;p&gt;Nova client help you to make all requests together, the most important one is the 4), if you are sure about all resources, you can simply run 4) to create an instance.&lt;/p&gt;

&lt;p&gt;The date flow will be follows:&lt;/p&gt;

&lt;p&gt;Nova client -&amp;gt; nova-api -&amp;gt; nova-conductor-&amp;gt;nova-scheduler-&amp;gt;nova-compute&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/boot.png&quot; alt=&quot;boot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each one is a service(a running process), they call each other through rpc server.&lt;/p&gt;

&lt;p&gt;Let’s take a look at the service side code.&lt;/p&gt;

&lt;p&gt;The nova-api service is a wigi server, you can find the code from:&lt;/p&gt;

&lt;p&gt;nova /api/openstack/compute/servers.py&lt;/p&gt;

&lt;p&gt;The handle function is&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def create(self, req, body):
    &quot;&quot;&quot;Creates a new server for a given user.&quot;&quot;&quot;
…   
(instances, resv_id) = self.compute_api.create(context,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After doing so many check, it finally calls the function of create() which defined in compute_api&lt;/p&gt;

&lt;p&gt;The code locates at nova/compute/api.py&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    @hooks.add_hook(&quot;create_instance&quot;)
def create(self, context, instance_type,
...
        return self._create_instance(
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;some validation and option are done in _create_instance function.
Then calling&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;self.compute_task_api.build_instances(context,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;compute_task_api is a rpc request to nova-conductor service.&lt;/p&gt;

&lt;p&gt;Go to nova/conductor/manager.py, We will find the function&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def build_instances(self, context, instances, image, filter_properties,
… 
       hosts = self.scheduler_rpcapi.select_destinations(context,
…
       self.compute_rpcapi.build_and_run_instance(context,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It mainly does 2 things, send request to nova-scheduler to find a host which the newly created instances will run on, and send request to these hosts’ to run an instance.&lt;/p&gt;

&lt;p&gt;Then we go to nova/compute/manager.py, which will receive the request from conductor , and it&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def build_and_run_instance(self, context, instance, image, request_spec,
                 filter_properties, admin_password=None,
                 injected_files=None, requested_networks=None,
                 security_groups=None, block_device_mapping=None,
                 node=None, limits=None):
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;this function will call the really hypervisor driver to create instance.
each service does its own job.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;nova-api: accept the url request and response to client&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;nova-conductor: talk to db, improve the security of db access&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;nova-scheduler: find a specify node though some require specification.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;nova-compute: do the really compute node related job.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;not all of request will call all the services.&lt;/p&gt;

&lt;h3 id=&quot;shelve-an-instance&quot;&gt;Shelve an instance&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;[tagett@stack-01 devstack]$ nova list
+--------------------------------------+-----------+-------------------+------------+-------------+-----------------------------------+
| ID                                   | Name      | Status            | Task State | Power State | Networks                          |
+--------------------------------------+-----------+-------------------+------------+-------------+-----------------------------------+
| 00be783d-bef5-46b1-bfdc-316618c76e92 | t2        | ACTIVE            | -          | Running     | private=192.168.1.82              |
+--------------------------------------+-----------+-------------------+------------+-------------+-----------------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then shelve an instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[tagett@stack-01 devstack]$ nova --debug shelve t2
REQ: curl -i 'http://cloudcontroller:5000/v2.0/tokens' …
REQ: curl -i 'http://cloudcontroller:8774/v2/d7beb7f28e0b4f41901215000339361d/servers'…
REQ: curl -i 'http://cloudcontroller:8774/v2/d7beb7f28e0b4f41901215000339361d/servers/r'…

…
REQ: curl -i 'http://cloudcontroller:8774/v2/d7beb7f28e0b4f41901215000339361d/servers/00be783d-bef5-46b1-bfdc-316618c76e92/action' -X POST -H &quot;Accept: application/json&quot; -H &quot;Content-Type: application/json&quot; -H &quot;User-Agent: python-novaclient&quot; -H &quot;X-Auth-Project-Id: admin&quot; -H &quot;X-Auth-Token: {SHA1}0634ea0ef1c3994e1f496c5d8890d32610cf11e9&quot; -d '{&quot;shelve&quot;: null}'…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are 4 requests you can see from above, you will see the full request url with request body and also you can find the response code body.&lt;/p&gt;

&lt;p&gt;The requests are:&lt;/p&gt;

&lt;p&gt;1)  Get token from keystone server with username and password, keystone will return the token, and also the tenant id.&lt;/p&gt;

&lt;p&gt;2)  Send another request with tenant id (d7beb7f28e0b4f41901215000339361d) and token. You will get all instances the tenant created.&lt;/p&gt;

&lt;p&gt;3)  Another request is check if the instance you want to shelve (00be783d-bef5-46b1-bfdc-316618c76e92) exist.&lt;/p&gt;

&lt;p&gt;4)  Do shelve, you can see it is a post request with the body  -d ‘{“shelve”: null}’&lt;/p&gt;

&lt;p&gt;At last , nova api return a response as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RESP: [202] CaseInsensitiveDict({'date': 'Thu, 18 Sep 2014 04:03:09 GMT', 'content-length': '0', 'content-type': 'text/html; charset=UTF-8', 'x-compute-request-id': 'req-4be7dc9a-21da-4050-9310-3ee58ca93569'}) RESP BODY: null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Server has accepted(202) your request and will do the shelve asynchronous.&lt;/p&gt;

&lt;p&gt;This is what you can see from client side.&lt;/p&gt;

&lt;p&gt;Then here’s the server side.&lt;/p&gt;

&lt;p&gt;The basic service calling sequence is:
Novaclient -&amp;gt; nova-api -&amp;gt; nova-conductor -&amp;gt; nova-compute&lt;/p&gt;

&lt;p&gt;the api code of shelve is in nova/api/openstack/compute/contrib/shelve.py
it is an extension of server(as you can see from the request of shelve)
you can find the follow code:&lt;/p&gt;

&lt;p&gt;nova/api/openstack/compute/contrib/shelve.py :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@wsgi.action('shelve')
def _shelve(self, req, id, body):
    &quot;&quot;&quot;Move an instance into shelved mode.&quot;&quot;&quot;
    context = req.environ[&quot;nova.context&quot;]
    auth_shelve(context)

    instance = self._get_instance(context, id)
    try:
        self.compute_api.shelve(context, instance)
    except exception.InstanceIsLocked as e:
        raise exc.HTTPConflict(explanation=e.format_message())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the api service will call nova compute api ‘s function shelve,
the code located at nova/compute/api.py&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    if not self.is_volume_backed_instance(context, instance):
        name = '%s-shelved' % instance['display_name']
        image_meta = self._create_image(context, instance, name,
                'snapshot')
        image_id = image_meta['id']
        self.compute_rpcapi.shelve_instance(context, instance=instance,
                image_id=image_id)
    else:
        self.compute_rpcapi.shelve_offload_instance(context,
                instance=instance)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then it make a rpc call though compute_rpcapi, the message will send to amqp server and 
it will received by compute manager, to let’s check the code in nova/compute/manager.py&lt;/p&gt;

&lt;p&gt;so the data flow will be&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/shelve.png&quot; alt=&quot;shevle&quot; /&gt;&lt;/p&gt;
</description>
                <link>http://taget.github.io/lessons/2014/09/18/request-data-flow</link>
                <guid>http://taget.github.io/lessons/2014/09/18/request-data-flow</guid>
                <pubDate>2014-09-18T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Deploy Openstack With Packstack</title>
                <description>&lt;h2 id=&quot;使用packstack部署openstack&quot;&gt;使用packstack部署openstack&lt;/h2&gt;

&lt;p&gt;本文将介绍如何使用Redhat提供的packstack部署openstack&lt;/p&gt;

&lt;h3 id=&quot;openstack-简介&quot;&gt;Openstack 简介&lt;/h3&gt;
&lt;p&gt;openstack 是一整套资源管理软件的集合，也是当前最热的开源虚拟化管理软件之一，有一个全球139个国家将近两万开发者参与的开源社区（www.openstack.org）作为支持。openstack项目的目的是快速建设一个稳定可靠的公有云或私有云系统。整个项目涵盖了计算，存储，网络以及前端展现等关于云管理的全部方面，包含了众多子项目。&lt;/p&gt;

&lt;p&gt;主要包含了一下几个子项目:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OpenStack Compute (code-name Nova) 计算服务&lt;/li&gt;
  &lt;li&gt;OpenStack Networking (code-name Neutron) 网络服务&lt;/li&gt;
  &lt;li&gt;OpenStack Object Storage (code-name Swift) 对象存储服务&lt;/li&gt;
  &lt;li&gt;OpenStack Block Storage (code-name Cinder) 块设备存储服务&lt;/li&gt;
  &lt;li&gt;OpenStack Identity (code-name Keystone) 认证服务&lt;/li&gt;
  &lt;li&gt;OpenStack Image Service (code-name Glance) 镜像文件服务&lt;/li&gt;
  &lt;li&gt;OpenStack Dashboard (code-name Horizon) 仪表盘服务&lt;/li&gt;
  &lt;li&gt;OpenStack Telemetry (code-name Ceilometer) 告警服务&lt;/li&gt;
  &lt;li&gt;OpenStack Orchestration (code-name Heat) 流程服务&lt;/li&gt;
  &lt;li&gt;OpenStack Database (code-name Trove) 数据库服务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;openstack的特点各个服务之间通过统一的REST风格的API调用，实现系统的松耦合。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/openstack-whole.jpg&quot; alt=&quot;whole picture of openstack&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图中实线代表client api调用，虚线代表各个组件之间通过REST API进行通信。从图中可以看出，openstack的整体架构是松耦合的，这样做的好处是，各个组件的开发人员可以只关注各自的领域，对各自领域的修改不会影响到其他开发人员。不过，从另一方面来讲，这种松耦合的架构也给整个系统的维护带来了一定的困难。运维人员可能要掌握更多的系统相关的知识去调试出了问题的组件。
关于openstack更多信息，读者可以查阅参考文献中的opesntack社区连接，用以获得更多最新的动态。&lt;/p&gt;

&lt;h3 id=&quot;部署openstack&quot;&gt;部署openstack&lt;/h3&gt;

&lt;p&gt;各种发行版提供了各自openstack的安装部署方式，基于fedora 的RDO，我们可以快速部署openstack环境到一个物理机上，然后对其进行计算节点扩展。&lt;/p&gt;

&lt;p&gt;RDO是一个快速安装部署fedora，redhat，centos发行版的openstack工具。另外，redhat也提供了企业版本openstack支持，参考如下链接http://www.redhat.com/openstack/&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;使用RDO部署单节点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;笔者使用环境是fedora20，只需要简单几条命令即可完成openstack的penstack的allinone模式，即部署openstack环境到一台单机上（fedora20环境下要打开Selinux, ‘sudo setenforce permissive’)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum update –y
sudo yum install -y openstack-packstack
packstack --allinone
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;packstack 配置过程中需要安装一些软件，比如说puppet，所以需要确保主机接入了网络，
同时确保sshd service是启动状态，按照提示输入root账号的密码。
packstack会帮我们创建mysql数据库，创建用户密码等，如果整个安装过程没有报错，整个环境就搭建好了。&lt;/p&gt;

&lt;p&gt;fedora20的openstack-puppet-modules-2013.2-9.1.fc20.noarch包中有一个bug，社区已经有了fix，读者可以自行按照该链接进行修改
/usr/share/openstack-puppet/modules/neutron/lib/puppet/provider/neutron_subnet/neutron.rb
https://git.openstack.org/cgit/stackforge/puppet-neutron/commit/?id=78428c844f73bf585a8f8a3bdf615ba2a0e8983b
安装过程中如果出现任何错误，读者可以根据屏幕输出的提示查看日志文件，手动修复后，删除生成的answers文件，重新运行packstack –allinone 即可。
如果一切安装顺利，在root目录下会生成以下3个文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;keystonerc_admin  keystonerc_demo   packstack-answers-20140624-142948.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;关于packstack的一些已知的workaroud，请查阅如下链接http://openstack.redhat.com/Workarounds&lt;/li&gt;
    &lt;li&gt;fedora 20 的openstack-dashboard 与 python-django 1.6.0 以上的版本不兼容，所以需要使用1.5版本的 python-django-1.5.5-2.fc20.noarch，这时，先引入packstack生成的环境变量文件&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;[root@stack-kvm110] # source .keystonerc_admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用nova-manage service list 命令查看nova服务的状态。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@stack-kvm110 ~(keystone_admin)]# nova-manage service list
Binary           Host                Zone            Status     State Updated_At
nova-conductor   stack-kvm110        internal          enabled    :-)   2014-06-27 02:12:09
nova-cert        stack-kvm110         internal         enabled    :-)   2014-06-27 02:12:07
nova-consoleauth stack-kvm110         internal          enabled    :-)   2014-06-27 02:12:08
nova-scheduler   stack-kvm110         internal          enabled    :-)   2014-06-27 02:12:12
nova-compute    stack-kvm110         nova            enabled    :-)   2014-06-27 02:12:13
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从命令输出可以看出在stack-kvm主机上运行的服务。
或者通过浏览器访问dashboard，在浏览器打开http://$YOURIP/dashboard 就可以访问安装好的openstack前端dashboard。用户名是admin，密码通过查看keystonerc_admin
可以获得，此时，控制节点，网络节点和计算节点都安装在一台服务器上了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;添加新的计算点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;接下来继续添加新的节点用做计算节点（9.181.129.64）。
打开packstack生成的packstack-answers-20140624-142948.txt 文件，编辑
修改网卡名字，为了让新添加的节点可以与之前创建的节点共享同一个似有网
需要修改CONFIG_NOVA_COMPUTE_PRIVIF和CONFIG_NOVA_NETWORK_PRIVIF，把他们指向一个物理网卡（第二块网卡）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Private interface for Flat DHCP on the Nova compute servers
CONFIG_NOVA_COMPUTE_PRIVIF=eth1
# Private interface for network manager on the Nova network server
CONFIG_NOVA_NETWORK_PRIVIF=eth1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，修改&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# A comma separated list of IP addresses on which to install the Nova
# Compute services
CONFIG_NOVA_COMPUTE_HOSTS=192.168.1.64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时保证 CONFIG_NOVA_NETWORK_HOSTS 是第一个节点192.168.1.110。
重新运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo packstack --answer-file=packstack-answers-20140624-142948.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装完成后，在controler节点上通过nova-manage service 可以看到新增加的compute节点stack-01。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@stack-kvm110 ~(keystone_admin)]# nova-manage service list
Binary           Host                     Zone             Status     State Updated_At
nova-conductor   stack-kvm110             internal         enabled    :-)   2014-06-27 02:23:29
nova-cert        stack-kvm110             internal         enabled    :-)   2014-06-27 02:23:28
nova-consoleauth stack-kvm110             internal         enabled    :-)   2014-06-27 02:23:28
nova-scheduler   stack-kvm110             internal         enabled    :-)   2014-06-27 02:23:32
nova-compute     stack-kvm110            nova          enabled    :-)   2014-06-27 02:23:33
**nova-compute     stack-01               nova          enabled    :-)   2014-06-27 02:23:27**
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置网络节点和计算节点的物理网络&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;本文中使用openvswitch 作为虚拟网络的驱动插件，图2是网络节点与单台计算节点的物理连接图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/neutron-phy.jpg&quot; alt=&quot;neutron 网络物理连接&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如图所示，绿色的连接是数据网络，专门用于租户的虚拟机之间的连接。灰色的是管理网络，用于openstack各个组件之间的网络通信。管理节点通过eth2网卡接入到外网路由器，也是整个openstack系统外网流量的出口。网络节点以及计算节点的neutron配置。&lt;/p&gt;

&lt;p&gt;本文中使用neutron配置虚拟网络，网络节点与控制节点安装在同一节点机器。
从图中我们可以看出openstack的网络分为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;管理网，用于openstack 各个组件之间的通信&lt;/li&gt;
  &lt;li&gt;数据网， 虚拟机之间的通信&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;vlan方式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;使用这种方式的前提是，连接计算节点和网络节点的交换机配置了或者支持vlan功能。
网络节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini：
[ovs]
tenant_network_type = vlan
network_vlan_ranges = physnet1,physnet2:101:110
integration_bridge = br-int
bridge_mappings = physnet1:br-ex,physnet2:br-eth1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;计算节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ovs]
tenant_network_type = vlan
network_vlan_ranges = physnet2:101:110
integration_bridge = br-int
bridge_mappings = physnet2:br-eth1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;图3中蓝框所示为openvswitch网桥&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/logic-network.jpg&quot; alt=&quot;虚拟网络的逻辑连接&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;gre方式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;与vlan方式类似，只不过在网络节点与计算节点之间建立了条专用通道
网络节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini：
enable_tunneling = True
tunnel_type = gre
tunnel_id_ranges = 1:1000
local_ip = 9.181.129.110
integration_bridge = br-int
tunnel_bridge = br-tun
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;计算节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;enable_tunneling = True
tunnel_type = gre
tunnel_id_ranges = 1:1000
local_ip = 9.181.129.64
integration_bridge = br-int
tunnel_bridge = br-tun
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用neutron命令创建虚拟网络
创建public网络，子网以及路由器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ neutron router-create router01
$ neutron net-create public01 \
--provider:network_type flat \
--provider:physical_network physnet1 \
--router:external=True
$ neutron subnet-create --name public01_subnet01 \
--gateway 192.168.1.1 public01 192.168.1.0、24 --disable-dhcp
$ neutron router-gateway-set router01 public01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建内部网络net01以及子网&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ neutron net-create net01 \
--provider:network_type vlan \
--provider:physical_network physnet2 \
--provider:segmentation_id 101
$ neutron subnet-create --tenant-id $tenant --name net01_subnet01 net01 192.168.101.0/24
$ neutron router-interface-add router01 net01_subnet01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同理创建内部网络net02以及子网&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ neutron net-create net02 \
--provider:network_type vlan \
--provider:physical_network physnet2 \
--provider:segmentation_id 102
$ neutron subnet-create --tenant-id $tenant --name net02_subnet01 net01 192.168.102.0/24
$ neutron router-interface-add router01 net02_subnet01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建基于gre的私有网络net03和子网以及net04和子网&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ neutron net-create net03 --provider:network_type gre --provider:physical_network physnet2 tunnel_id_ranges = 1:1000 enable_tunneling = True
$ neutron subnet-create --name net03_subnet01 net03 192.168.103.0/24

$ neutron net-create net04 --provider:network_type gre --provider:physical_network physnet2 tunnel_id_ranges = 1:1000 enable_tunneling = True
$ neutron subnet-create --name net04_subnet01 Net04 192.168.104.0/24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dashboard.jpg&quot; alt=&quot;openstack dashboard&quot; /&gt;&lt;/p&gt;

</description>
                <link>http://taget.github.io/lessons/2014/08/14/deploy-openstack-with-packstack</link>
                <guid>http://taget.github.io/lessons/2014/08/14/deploy-openstack-with-packstack</guid>
                <pubDate>2014-08-14T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Linux Interrupt</title>
                <description>
&lt;h2 id=&quot;linux-中断&quot;&gt;Linux 中断&lt;/h2&gt;
&lt;p&gt;中断硬件发起，外部设备与cpu通信的一种方式，相对于cpu轮询，中断是更好的方式。
中断设备与cpu的物理连接方式如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pic-cpu-connection.jpeg&quot; alt=&quot;pci bridge connection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一般来说，外设并不直接与cpu相连接，而是通过一个叫’中断控制器’硬件连接。这个中断控制器是可被编程配置的。
所以又称它为’可编程中断控制器’（PIC）。外设向中断控制器发起中断，然后，’中断控制器’告诉cpu有中断到来。
cpu去读取中断控制器（PIC）产生的软中断号。
初始化需要配置PIC，配置内容包括：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;中断触发的电信号类型（水平还是边沿触发）&lt;/li&gt;
  &lt;li&gt;将外设的引脚编号映射到cpu可见的软件中断号irq&lt;/li&gt;
  &lt;li&gt;屏蔽掉某些外部设备的中断触发&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;中断向量表&quot;&gt;中断向量表&lt;/h2&gt;

&lt;p&gt;中断向量表是cpu的内部概念。当发生 ‘异常’ 或者 ‘内部中断’时，cpu会查看中断向量表该异常或者中断对应的处理函数地址。&lt;/p&gt;

&lt;h2 id=&quot;通用中断处理函数&quot;&gt;通用中断处理函数&lt;/h2&gt;

&lt;p&gt;通常，当发生外部设备中断时，外设向中断控制器产生一个中断，中断控制器生成一个软件中断信号通知cpu。cpu发现是一个外部
中断，他会查询中断向量表中的某一表项去处理该中断。该表项对应的就是通用中断处理函数的入口地址。操作系统会提供这个通
用中断处理函数的定义。&lt;/p&gt;

&lt;p&gt;通用中断处理函数的处理过程大致为：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;获取软件中断号irq，一般使用汇编编写。&lt;/li&gt;
  &lt;li&gt;调用do_irq，c函数编写&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以，当一个外部设备产生中断后，到cpu响应该中断的流程如下：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;外设触发一个中断&lt;/li&gt;
  &lt;li&gt;中断控制器（PIC）获取中断号并报告给cpu&lt;/li&gt;
  &lt;li&gt;cpu得知这是一个外部中断，它去查找他的中断向量表，找到通用中断处理函数的地址&lt;/li&gt;
  &lt;li&gt;cpu调用中断处理函数，通过向PIC查询软中断号&lt;/li&gt;
  &lt;li&gt;中断处理函数通过软中断号找到该外设注册的中断处理子函数（由驱动提供,在irq_desc数组中）&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;hardirq-和-softirq&quot;&gt;HARDIRQ 和 SOFTIRQ&lt;/h2&gt;

&lt;p&gt;当cpu在响应一个中断的时候，如果有新的中断到来了，如何处理呢？
cpu为了处理这种情况，将中断处理分成了上半部和下半部。
HARDIRQ在中断关闭情况下执行，他的执行时间尽可能短
SOFTIRQ在中断开启的情况下执行。此时外部设备仍可以继续中断处理器，一般驱动程序可以将耗时部分放在此部分执行。&lt;/p&gt;

</description>
                <link>http://taget.github.io/2014/06/13/linux-interrupt</link>
                <guid>http://taget.github.io/2014/06/13/linux-interrupt</guid>
                <pubDate>2014-06-13T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Use Cloud Image Locally</title>
                <description>&lt;h1 id=&quot;如何在本地使用cloud镜像&quot;&gt;如何在本地使用cloud镜像&lt;/h1&gt;

&lt;p&gt;fedora，ubuntu等发行版厂商提供了各自cloud的镜像文件，我们可以直接下载并将其运行为一个虚拟机实例。
但是这些cloud镜像文件默认磁盘分区大小可能无法满足我们的需求，而且，默认是不提供通过密码访问的方式访问虚拟机。
本文将介绍如何快速修该磁盘分区大小以及访问虚拟机。
本文中使用的环境为fedora 20，并且需要libguestfs-tools提供的工具，使用如下命令安装。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    sudo yum install libguestfs-tools -y
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;首先获取image使用国内163网站的镜像服务器下载fedora20的镜像文件&quot;&gt;首先获取image，使用国内163网站的镜像服务器下载fedora20的镜像文件:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;    wget https://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2.xz

    xz -v --decompress CentOS-7-x86_64-GenericCloud.qcow2.xz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为libguestfs工具以来libvirt启动虚拟机来进行编辑镜像文件，所以我们要把镜像文件放在/var/lib/libvirt/images目录下。&lt;/p&gt;

&lt;h3 id=&quot;查看镜像文件大小并对其进行扩展&quot;&gt;查看镜像文件大小，并对其进行扩展&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ virt-filesystems --long --parts --blkdevs -h -a CentOS-7-x86_64-GenericCloud.qcow2
Name       Type       MBR  Size  Parent
/dev/sda1  partition  83   1.9G  /dev/sda
/dev/sda   device     -    2.0G  -
$ virt-df -h  CentOS-7-x86_64-GenericCloud.qcow2
Filesystem                                Size       Used  Available  Use%
Fedora-x86_64-20-20131211.1-sda.qcow2:/dev/sda1
                                      1.8G       572M       1.3G   31%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;libguestfs不支持in-place修改，所以使用qemu-img命令创建一个30G的磁盘，然后扩展原始镜像，最终会创建一个新的镜像文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ qemu-img create -f qcow2 new-disk 30G
Formatting 'new-disk', fmt=qcow2 size=32212254720 encryption=off cluster_size=65536 lazy_refcounts=off

$ virt-resize CentOS-7-x86_64-GenericCloud.qcow2 new-disk --expand /dev/sda1
Examining CentOS-7-x86_64-GenericCloud.qcow2 ...
100% ⟦▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒⟧ --:--
**********

Summary of changes:

/dev/sda1: This partition will be resized from 1.9G to 30.0G.  The
filesystem ext4 on /dev/sda1 will be expanded using the 'resize2fs'
method.

**********
Setting up initial partition table on new-disk ...
Copying /dev/sda1 ...
 100% ⟦▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒⟧ 00:00
Expanding /dev/sda1 using the 'resize2fs' method ...

Resize operation completed with no errors.  Before deleting the old
disk, carefully check that the resized disk boots and works correctly
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果过程中无错误，原始镜像被扩展到new-disk镜像文件中，再次查看磁盘分区大小,sda1分区变为30G&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ virt-df -h new-disk
Filesystem                                Size       Used  Available  Use%
new-disk:/dev/sda1                         30G       573M        29G    2%
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;访问虚拟机&quot;&gt;访问虚拟机&lt;/h3&gt;
&lt;p&gt;cloud 镜像文件默认是无法使用密码登陆的，如果想访问虚拟机，如何去做呢？&lt;/p&gt;

&lt;p&gt;有两种方法:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一种是通过libguestfs 提供的工具修改镜像文件的文件系统&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;打开文件访问功能并设置密码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ virt-customize -a new-disk --root-password password:kvm123456 \
--edit '/etc/ssh/sshd_config: s/PasswordAuthentication no/PasswordAuthentication yes/'
[   0.0] Examining the guest ...
[   5.0] Setting a random seed
[   5.0] Editing: /etc/ssh/sshd_config
[   5.0] Setting passwords
[   5.0] Finishing off
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者通过注入ssh publick key
libguestfs还提供了guestmount工具，用于把image镜像mount到某个目录,加入ssh publich key&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ guestmount -a new-disk -i /mnt/guest/
$ ls /mnt/guest/
bin  boot  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
$ touch /mnt/guest/etc/1
$ echo &amp;lt;&amp;lt; EOF &amp;gt;&amp;gt;/mnt/guest/root/.ssh/authorized_keys
{your public key }
EOF
$ guestunmount /mnt/guest/
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;第二种方式是通过 cloud-init&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;cloud-init 是ubuntu社区的一个项目，当虚拟机系统启动时自动运行，与openstack neutron 配合，通过dhcp服务获取虚拟机ip，访问网络节点
的80端口获得用户配置的资源，用于配置当前环境。
本文中我们没有创建用于cloud-init的服务器，当cloud-init没有找到服务器之后，它还会继续探测是否加载了cd-rom（/dev/sr0）设备。
所以，我们可以准备一个小的用于cloud-init访问的iso文件，在里面配置我们用到的密码或者sshkey。
具体代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;gt; meta-data &amp;lt;&amp;lt; END
instance-id: iid-local01;
local-hostname: myhost;
END

$ cat &amp;gt; user-data &amp;lt;&amp;lt; END
#cloud-config
password: mypassword
ssh_pwauth: True
chpasswd: { expire: False }

ssh_authorized_keys:
  - ssh-rsa {(insert ~/.ssh/id_rsa.pub here)}
END
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请注意格式。&lt;/p&gt;

&lt;p&gt;生成iso文件，并挂载到虚拟机上。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    genisoimage -output init.iso -volid cidata -joliet -rock user-data meta-data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把iso添加成cdrom设备：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    &amp;lt;disk type='file' device='cdrom'&amp;gt;
        &amp;lt;driver name='qemu' type='raw'/&amp;gt;
        &amp;lt;source file='/home/taget/init.iso'/&amp;gt;
        &amp;lt;target dev='hdc' bus='ide'/&amp;gt;
        &amp;lt;readonly/&amp;gt;
        &amp;lt;address type='drive' controller='0' bus='1' target='0' unit='0'/&amp;gt;
    &amp;lt;/disk&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动虚拟机后，cloud-init会自动运行并配置主机。&lt;/p&gt;

&lt;p&gt;关于cloud-init的其他配置使用情况可以参考&lt;a href=&quot;http://cloudinit.readthedocs.org/en/latest/&quot;&gt; Cloud init Document &lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;关于网络&quot;&gt;关于网络&lt;/h3&gt;

&lt;p&gt;如果使用ubuntu的cloud image，系统在启动过程中会扫描网络设备的名字，所以，网络设备可以正确启动。
如果是CentOS的image，在配置虚拟机interface的时候要指定为virtio驱动，保证系统发现的网络设备名称为eth0。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    &amp;lt;interface type='network'&amp;gt;
        &amp;lt;mac address='52:54:00:e2:fa:19'/&amp;gt;
        &amp;lt;source network='default'/&amp;gt;
        &amp;lt;model type='virtio'/&amp;gt;
        &amp;lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&amp;gt;
    &amp;lt;/interface&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考链接：
http://kimizhang.wordpress.com/2014/03/18/how-to-inject-filemetassh-keyroot-passworduserdataconfig-drive-to-a-vm-during-nova-boot/
https://www.technovelty.org/linux/running-cloud-images-locally.html
http://cloudinit.readthedocs.org/en/latest/&lt;/p&gt;
</description>
                <link>http://taget.github.io/lessons/2014/06/10/use-cloud-image-locally</link>
                <guid>http://taget.github.io/lessons/2014/06/10/use-cloud-image-locally</guid>
                <pubDate>2014-06-10T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Linux Network Namespace</title>
                <description>&lt;h2 id=&quot;linux-network-namespaces&quot;&gt;Linux Network Namespaces&lt;/h2&gt;

&lt;p&gt;Linux kernel在2.6.29中加入了namespaces，用于支持网络的隔离，我们看一下namespace是如何使用的&lt;/p&gt;

&lt;h3 id=&quot;创建与配置&quot;&gt;创建与配置&lt;/h3&gt;

&lt;p&gt;创建一个名为blue的namespace&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip netns add blue
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;列出所有的namespace&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip netns list
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;分配网络接口到namespace上&quot;&gt;分配网络接口到namespace上&lt;/h3&gt;

&lt;p&gt;我们可以将一对veth中的一个分配到namespace上，将另一个分配到另一个上。&lt;/p&gt;

&lt;p&gt;veth的创建如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip link add veth0 type veth peer name veth1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就创建了一对veth，veth0 和veth1。
veth的作用就像一根网线一样，从一端进入的数据会从另一端出来。
使用&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip link list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看创建的veth设备。&lt;/p&gt;

&lt;p&gt;如果我们想把刚创建的namaespace与global/default namespace连接，我们可以这样做：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip link set veth1 netns blue
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;veth1 从global从消失了,因为这对veth的另一端veth0在default中，这样我们就可以将两个 namespace联系起来了。&lt;/p&gt;

&lt;p&gt;使用如下命令查看blue namespace中的连接&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip netns exec blue ip link list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如何配置namespace中的接口呢？可以使用如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip netns exec &amp;lt;network namespace&amp;gt; &amp;lt;command to run against that namespace&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;连接到物理网络&quot;&gt;连接到物理网络&lt;/h3&gt;

&lt;p&gt;可以使用linux bridge 或者 openvswith bridge。将物理接口和veth的一个加入到同一个bridge就可以了。&lt;/p&gt;

</description>
                <link>http://taget.github.io/lessons/2014/06/09/linux-network-namespace</link>
                <guid>http://taget.github.io/lessons/2014/06/09/linux-network-namespace</guid>
                <pubDate>2014-06-09T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Linux PCI</title>
                <description>
&lt;h1 id=&quot;pci-简介&quot;&gt;PCI 简介&lt;/h1&gt;
&lt;p&gt;PCI(Periheral Component Interconnect)有三种地址空间：PCI I/O空间、PCI内存地址空间和PCI配置空间。&lt;/p&gt;

&lt;p&gt;I/O 空间和内存地址空间由PCI驱动使用。
PCI配置空间由Linux PCI初始化代码使用，由内核用来进行配置，比如中断号和I/O内存基址空间。&lt;/p&gt;

&lt;h2 id=&quot;pci-桥&quot;&gt;PCI 桥&lt;/h2&gt;
&lt;p&gt;一共有三种PCI桥：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;HOST/PCI桥（HOST bridge）：用于连接CPU和PCI的桥设备&lt;/li&gt;
  &lt;li&gt;PCI/ISA桥 （ISA bridge）：用于连接PCI总线和ISA总线的桥&lt;/li&gt;
  &lt;li&gt;PCI-PCI桥（PCI-PCI bridge）：用于PCI总线的扩展
下图展示了pci桥的层次连接：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pci-connection.jpg&quot; alt=&quot;pci bridge connection&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在使用PCI设备之前,Linux Knerle需要对其进行枚举和配置。
枚举的结果就将是一个树状的结构，根是HOST/PCI桥。&lt;/p&gt;

&lt;h2 id=&quot;配置pci设备&quot;&gt;配置PCI设备&lt;/h2&gt;
&lt;p&gt;Linux kernel 对PCI配置空间进行配置。这个PCI配置空间其实就是一些寄存器，称为配置寄存器组。
当PCI设备尚未被激活的时候，它只对配置事务响应。设备上是不会有I/O端口映射到计算机的内存空间的。
中断也会被禁止。&lt;/p&gt;

&lt;h2 id=&quot;pci设备的地址空间&quot;&gt;PCI设备的地址空间&lt;/h2&gt;
&lt;p&gt;所有PCI设备的配置空间寄存器组都采用相同的地址，由总线的PCI桥在访问时附加上其他条件进行区分。
对于CPU来说，它通过一个统一的入口地址HOST/PCI桥，即树型结构的根发出指令，再由相应的PCI桥间接地完成具体的读写。
这个PCI桥（或者称PCI总线）包含两个寄存器，’地址寄存器’和’数据寄存器’，CPU向’地址寄存器’写入要访问的地址，然后通过’数据寄存器’读取数据。
地址寄存器的地址组成：
总线号：设备号：功能号：寄存器号
示例，lspci 输出：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;0000 : PCI domain (each domain can contain up to 256 PCI buses)&lt;/li&gt;
  &lt;li&gt;04   : the bus number the device is attached to&lt;/li&gt;
  &lt;li&gt;00   : the device number&lt;/li&gt;
  &lt;li&gt;.0   : PCI device function&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;访问pci设备的几种方式&quot;&gt;访问PCI设备的几种方式&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;通过访问I/O端口, I/O 地址空间一般不关联到系统内存，因为端口也可以映射到内存中，会引起混淆;&lt;/li&gt;
  &lt;li&gt;I/O 内存映射，就是我们通常说的MMIO,即将特定外设的端口地址映射到普通内存中。cpu可以像普通内存一样操作设备。内核通过ioremap和iounmap命令用于映射I/O内存&lt;/li&gt;
  &lt;li&gt;轮询和中断，显然中断是更好的方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pci的barbase-address-register&quot;&gt;PCI的BAR（base address register）&lt;/h2&gt;
&lt;p&gt;BAR是PCI配置空间中从0x10 到 0x24的6个register，用来定义PCI需要的配置空间大小以及配置PCI设备占用的地址空间。
X86的地址空间分为，IO 和 MEM 两类，PCI的BAR的某一位决定了设备需要映射到MEM空间还是IO空间。
POWERPC只有MEM空间（IO空间不单独编址）。&lt;/p&gt;

</description>
                <link>http://taget.github.io/2014/06/06/linux-pci</link>
                <guid>http://taget.github.io/2014/06/06/linux-pci</guid>
                <pubDate>2014-06-06T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Fedora Live Upgrade</title>
                <description>&lt;h1 id=&quot;fedora-在线升级&quot;&gt;Fedora 在线升级&lt;/h1&gt;
&lt;h2 id=&quot;所需工具工具-fedup&quot;&gt;所需工具工具 fedup&lt;/h2&gt;
&lt;h3 id=&quot;fedup的安装&quot;&gt;fedup的安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$sudo yum install fedup
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;fedup装成功后，推荐使用网络方式升级系统&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$sudo fedup-cli --network 20 --addrepo f20=http://mirrors.163.com/fedora/releases/20/Everything/x86_64/os/
&lt;/code&gt;&lt;/pre&gt;
</description>
                <link>http://taget.github.io/lessons/2014/06/06/fedora-live-upgrade</link>
                <guid>http://taget.github.io/lessons/2014/06/06/fedora-live-upgrade</guid>
                <pubDate>2014-06-06T00:00:00+08:00</pubDate>
        </item>


</channel>
</rss>
